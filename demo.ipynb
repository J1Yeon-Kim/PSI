{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating 3D People in Scenes without People\n",
    "\n",
    "Here we give a frontend demo of how to generate body meshes in a scene without people. \n",
    "+ First, we use a pre-trained conditional VAE model to generate body meshes. Here we only show the one-stage model without scene loss. \n",
    "+ Second, we perform scene geometry-aware fitting.\n",
    "\n",
    "The code in this demo is slightly different from the code in other places. __To efficiently generate a large amount of body meshes for various scenes, we recommend to use the frontend sh scripts.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) loading dependencies, models and setup environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "import sys, os, glob\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "# proj_path = '/is/ps2/yzhang/workspaces/PSI-internal'\n",
    "proj_path = '/home/yzhang/workspaces/smpl-env-gen-3d-internal'\n",
    "sys.path.append(proj_path)\n",
    "sys.path.append(proj_path+'/source')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import smplx\n",
    "from human_body_prior.tools.model_loader import load_vposer\n",
    "\n",
    "from cvae import HumanCVAES1, HumanCVAES2, ContinousRotReprDecoder\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smplx\n",
    "from human_body_prior.tools.model_loader import load_vposer\n",
    "import chamfer_pytorch.dist_chamfer as ext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put some auxilary functions here, mainly for coordinate transform and file parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.6.8 64-bit.",
     "traceback": [
      "Error: Jupyter cannot be started. Error attempting to locate jupyter: Data Science libraries jupyter and notebook are not installed in interpreter Python 3.6.8 64-bit.",
      "at C.startServer (/home/yzhang/.vscode-server/extensions/ms-python.python-2020.3.71659/out/client/extension.js:1:776170)",
      "at processTicksAndRejections (internal/process/task_queues.js:89:5)",
      "at async C.ensureServerAndNotebookImpl (/home/yzhang/.vscode-server/extensions/ms-python.python-2020.3.71659/out/client/extension.js:1:775625)",
      "at async C.ensureServerAndNotebook (/home/yzhang/.vscode-server/extensions/ms-python.python-2020.3.71659/out/client/extension.js:1:775315)",
      "at async C.clearResult (/home/yzhang/.vscode-server/extensions/ms-python.python-2020.3.71659/out/client/extension.js:1:771356)",
      "at async C.reexecuteCell (/home/yzhang/.vscode-server/extensions/ms-python.python-2020.3.71659/out/client/extension.js:1:760721)",
      "at async C.reexecuteCells (/home/yzhang/.vscode-server/extensions/ms-python.python-2020.3.71659/out/client/extension.js:1:757735)"
     ]
    }
   ],
   "source": [
    "def recover_global_T(x_batch, cam_intrisic, max_depth):\n",
    "    xt_batch = x_batch[:,:3]\n",
    "    xr_batch = x_batch[:,3:]\n",
    "\n",
    "    fx_batch = cam_intrisic[:,0,0]\n",
    "    fy_batch = cam_intrisic[:,1,1]\n",
    "    px_batch = cam_intrisic[:,0,2]\n",
    "    py_batch = cam_intrisic[:,1,2]\n",
    "    s_ = 1.0 / torch.max(px_batch, py_batch)\n",
    "\n",
    "    z = (xt_batch[:, 2]+1.0)/2.0 * max_depth\n",
    "\n",
    "    x = xt_batch[:,0] * z / s_ / fx_batch\n",
    "    y = xt_batch[:,1] * z / s_ / fy_batch\n",
    "    \n",
    "    xt_batch_recoverd = torch.stack([x,y,z],dim=-1)\n",
    "\n",
    "    return torch.cat([xt_batch_recoverd, xr_batch],dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_3D_rot(x_batch):\n",
    "    xt = x_batch[:,:3]\n",
    "    xr = x_batch[:,3:9]\n",
    "    xb = x_batch[:,9:]\n",
    "\n",
    "    xr_mat = ContinousRotReprDecoder.decode(xr) # return [:,3,3]\n",
    "    xr_aa = ContinousRotReprDecoder.matrot2aa(xr_mat) # return [:,3]\n",
    "\n",
    "    return torch.cat([xt, xr_aa, xb], dim=-1)\n",
    "\n",
    "\n",
    "def body_params_encapsulate(x_body_rec, to_numpy=True, batched=False):\n",
    "    \n",
    "    if to_numpy:\n",
    "        x_body_rec_np = x_body_rec.detach().cpu().numpy()\n",
    "    else:\n",
    "        x_body_rec_np = x_body_rec\n",
    "        \n",
    "    \n",
    "    if batched:\n",
    "        body_params_batch_rec={}\n",
    "        body_params_batch_rec['transl'] = x_body_rec_np[:,:3]\n",
    "        body_params_batch_rec['global_orient'] = x_body_rec_np[:,3:6]\n",
    "        body_params_batch_rec['betas'] = x_body_rec_np[:,6:16]\n",
    "        body_params_batch_rec['body_pose'] = x_body_rec_np[:,16:48]\n",
    "        body_params_batch_rec['left_hand_pose'] = x_body_rec_np[:,48:60]\n",
    "        body_params_batch_rec['right_hand_pose'] = x_body_rec_np[:,60:]\n",
    "        \n",
    "        return body_params_batch_rec\n",
    "    \n",
    "    else:\n",
    "        n_batch = x_body_rec_np.shape[0]\n",
    "        rec_list = []\n",
    "\n",
    "        for b in range(n_batch):\n",
    "            body_params_batch_rec={}\n",
    "            body_params_batch_rec['transl'] = x_body_rec_np[b:b+1,:3]\n",
    "            body_params_batch_rec['global_orient'] = x_body_rec_np[b:b+1,3:6]\n",
    "            body_params_batch_rec['betas'] = x_body_rec_np[b:b+1,6:16]\n",
    "            body_params_batch_rec['body_pose'] = x_body_rec_np[b:b+1,16:48]\n",
    "            body_params_batch_rec['left_hand_pose'] = x_body_rec_np[b:b+1,48:60]\n",
    "            body_params_batch_rec['right_hand_pose'] = x_body_rec_np[b:b+1,60:]\n",
    "            rec_list.append(body_params_batch_rec)\n",
    "\n",
    "        return rec_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_preprocessing(img, modality, target_domain_size=[128, 128]):\n",
    "\n",
    "    \"\"\"\n",
    "    input:\n",
    "        - img (depthmap or semantic map): [height, width].\n",
    "        - modality: 'depth' or 'seg'\n",
    "    output:\n",
    "        canvas: with shape of target_domain_size, where the input is in the\n",
    "                center tightly, with shape target_domain_size\n",
    "        factor: the resizing factor\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare the canvas\n",
    "    img_shape_o = img.shape\n",
    "    canvas = torch.zeros([1,1]+target_domain_size, dtype=torch.float32,\n",
    "                         device=torch.device(\"cuda\"))\n",
    "\n",
    "\n",
    "    # filter out unavailable values\n",
    "    if modality == 'depth':\n",
    "        img[img>6.0]=6.0\n",
    "\n",
    "    if modality == 'seg':\n",
    "        img[img>41] = 41\n",
    "\n",
    "\n",
    "\n",
    "    ## rescale to [-1,1]\n",
    "    max_val = torch.max(img)\n",
    "    _img = 2* img / max_val - 1.0\n",
    "\n",
    "    ## put _img to the canvas\n",
    "    if img_shape_o[0]>= img_shape_o[1]:\n",
    "        factor = float(target_domain_size[0]) / img_shape_o[0]\n",
    "        target_height = target_domain_size[0]\n",
    "        target_width = int(img_shape_o[1] * factor) //2 *2 \n",
    "\n",
    "        # for depth map we use bilinear interpolation in resizing\n",
    "        # for segmentation map we use bilinear interpolation as well.\n",
    "        # note that float semantic label is not real in practice, but\n",
    "        # helpful in our work\n",
    "        target_size = [target_height, target_width]\n",
    "\n",
    "        _img = _img.view(1,1,img_shape_o[0],img_shape_o[1])\n",
    "        img_resize = F.interpolate(_img, size=target_size, mode='bilinear',\n",
    "                                    align_corners=False)\n",
    "\n",
    "        na = target_width\n",
    "        nb = target_domain_size[1]\n",
    "        lower = (nb //2) - (na //2)\n",
    "        upper = (nb //2) + (na //2)\n",
    "\n",
    "        canvas[:,:,:, lower:upper] = img_resize\n",
    "\n",
    "\n",
    "    else:\n",
    "        factor = float(target_domain_size[1]) / img_shape_o[1]\n",
    "\n",
    "        target_height = int(factor*img_shape_o[0]) //2 *2\n",
    "        target_width = target_domain_size[1]\n",
    "\n",
    "        target_size = [target_height, target_width]\n",
    "        _img = _img.view(1,1,img_shape_o[0],img_shape_o[1])\n",
    "        img_resize = F.interpolate(_img, size=target_size, mode='bilinear',\n",
    "                                    align_corners=False)\n",
    "\n",
    "        na = target_height\n",
    "        nb = target_domain_size[0]\n",
    "        lower = (nb //2) - (na //2)\n",
    "        upper = (nb //2) + (na //2)\n",
    "\n",
    "        canvas[:,:,lower:upper, :] = img_resize\n",
    "\n",
    "    return canvas, factor, max_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scipy_matfile_parse(filename):\n",
    "    '''\n",
    "    parse data from files and put them to GPU\n",
    "    Note that this function is for demo, and is different from the ones used in other places.\n",
    "    '''\n",
    "    data = sio.loadmat(filename)\n",
    "    depth0_np = data['depth']\n",
    "    seg0_np = data['seg']\n",
    "\n",
    "    ## change them to torch tensor\n",
    "    depth0 = torch.tensor(depth0_np, dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "    seg0 = torch.tensor(seg0_np, dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "\n",
    "    ## pre_processing\n",
    "    depth, factor_d,max_d = data_preprocessing(depth0, 'depth', target_domain_size=[128, 128])\n",
    "    seg, factor_s,_ = data_preprocessing(seg0, 'seg', target_domain_size=[128, 128])\n",
    "\n",
    "\n",
    "    cam_intrinsic_np = data['cam'][0][0]['intrinsic']\n",
    "    cam_intrinsic = torch.tensor(cam_intrinsic_np, dtype=torch.float32, device=torch.device(\"cuda\")).unsqueeze(0)\n",
    "    cam_extrinsic_np = data['cam'][0][0]['extrinsic']\n",
    "    cam_extrinsic_np = np.linalg.inv(cam_extrinsic_np)\n",
    "    cam_extrinsic = torch.tensor(cam_extrinsic_np, dtype=torch.float32, device=torch.device(\"cuda\")).unsqueeze(0)\n",
    "\n",
    "    return depth, seg, max_d.view(1), cam_intrinsic, cam_extrinsic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Prepare the scene without people\n",
    "\n",
    "Our method requires the following data about a scene:\n",
    "+ depth map\n",
    "+ semantic segmentation\n",
    "+ the camera parameters (extrinsic and intrinsic)\n",
    "+ the scene signed distance function (SDF)\n",
    "+ the scene mesh\n",
    "\n",
    "Note that SDF and scene mesh are only used for scene-geometry aware fitting. For generating body meshes with the CVAE model, only the first three attributes are sufficient.\n",
    "\n",
    "Here we use the 'MPH16' scene in the __PROXE__ dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenename = 'MPH16'\n",
    "proxe_path = '/home/yzhang/Videos/PROXE'\n",
    "\n",
    "## read the depth and semantics\n",
    "scene_matfile_path = os.path.join(proxe_path, 'snapshot_for_testing/MPH16_00157_01/rec_000000.mat')\n",
    "depth, seg, max_d, cam_intrinsic, cam_extrinsic = scipy_matfile_parse(scene_matfile_path)\n",
    "\n",
    "## read the sdf\n",
    "with open(os.path.join(proxe_path, 'scenes_sdf',scenename+'.json')) as f:\n",
    "    sdf_data = json.load(f)\n",
    "    grid_min = np.array(sdf_data['min'])\n",
    "    grid_max = np.array(sdf_data['max'])\n",
    "    grid_dim = sdf_data['dim']\n",
    "sdf = np.load(os.path.join(proxe_path, 'scenes_sdf', scenename + '_sdf.npy')).reshape(grid_dim, grid_dim, grid_dim)\n",
    "\n",
    "## read the scene mesh\n",
    "scene_mesh = o3d.io.read_triangle_mesh(os.path.join(proxe_path, 'scenes_downsampled', scenename+'.ply'))\n",
    "scene_verts = np.asarray(scene_mesh.vertices)\n",
    "scene_faces = np.asarray(scene_mesh.triangles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3af0174ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## We could visualize the scene data, or skip this step.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "depth_processed_np = depth.detach().cpu().squeeze().numpy()\n",
    "plt.imshow(depth_processed_np)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "seg_processed_np = seg.detach().cpu().squeeze().numpy()\n",
    "plt.imshow(seg_processed_np)\n",
    "\n",
    "# # we use webGL to visualize 3D, which is a different case from running locally.\n",
    "# # only works for point cloud visualization\n",
    "# # note that visualizing 3D here may cause slow responses.\n",
    "# pcd = o3d.geometry.PointCloud()\n",
    "# pcd.points = scene_mesh.vertices\n",
    "# pcd.colors = scene_mesh.vertex_colors\n",
    "\n",
    "# from open3d import JVisualizer\n",
    "# visualizer = JVisualizer()\n",
    "# visualizer.add_geometry(pcd)\n",
    "# visualizer.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （3） Generating body meshes using the pre-trained conditional VAE model\n",
    "\n",
    "For demonstration purposes, we only use the **one-stage model without scene loss**. For other models, the pipeline is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Trained Model: /home/yzhang/body_models/VPoser/vposer_v1_0/snapshots/TR00_E096.pt\n",
      "[INFO] load checkpoints: checkpoints_v2/checkpoints_proxtrain_models1_batch32_epoch30_LR0.0003_LossVposer0.001_LossKL0.1_LossContact0.000001_LossCollision0.000001/epoch-000030.ckp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testconfig={\n",
    "    'smplx_model_path': '/home/yzhang/body_models/VPoser',\n",
    "    'scene_model_ckpt': '/home/yzhang/workspaces/smpl-env-gen-3d-internal/data/resnet18.pth',\n",
    "    'vposer_ckpt_path': '/home/yzhang/body_models/VPoser/vposer_v1_0',\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'ckpt_dir': 'checkpoints_v2/checkpoints_proxtrain_models1_batch32_epoch30_LR0.0003_LossVposer0.001_LossKL0.1_LossContact0.000001_LossCollision0.000001',\n",
    "    'n_samples': 5\n",
    "}\n",
    "\n",
    "### our conditional vae model\n",
    "model_h = HumanCVAES1(latentD=256, # default value in our checkpoints\n",
    "                      n_dim_body=75,# global T(3d) + global R(6d) + shape (10d) + pose (32d) + hand (24d)\n",
    "                      scene_model_ckpt=None,\n",
    "                      test=True)\n",
    "\n",
    "# model_h = HumanCVAES2(latentD_g=256, # default value in our checkpoints\n",
    "#                       latentD_l=256, # default value in our checkpoints\n",
    "#                       n_dim_body=75,# global T(3d) + global R(6d) + shape (10d) + pose (32d) + hand (24d)\n",
    "#                       scene_model_ckpt=None,\n",
    "#                       test=True)\n",
    "\n",
    "### VPoesr\n",
    "vposer, _ = load_vposer(testconfig['vposer_ckpt_path'], vp_model='snapshot')\n",
    "\n",
    "### smplx\n",
    "body_mesh_model = smplx.create(testconfig['smplx_model_path'], \n",
    "                               model_type='smplx',\n",
    "                               gender='neutral', ext='npz',\n",
    "                               num_pca_comps=12,\n",
    "                               create_global_orient=True,\n",
    "                               create_body_pose=True,\n",
    "                               create_betas=True,\n",
    "                               create_left_hand_pose=True,\n",
    "                               create_right_hand_pose=True,\n",
    "                               create_expression=True,\n",
    "                               create_jaw_pose=True,\n",
    "                               create_leye_pose=True,\n",
    "                               create_reye_pose=True,\n",
    "                               create_transl=True,\n",
    "                               batch_size=testconfig['n_samples']\n",
    "                               )\n",
    "\n",
    "## setup models and load checkpoints\n",
    "model_h.eval()\n",
    "model_h.to(testconfig['device'])\n",
    "\n",
    "vposer.to(testconfig['device'])\n",
    "body_mesh_model.to(testconfig['device'])\n",
    "\n",
    "ckp_path = sorted(glob.glob(os.path.join(testconfig['ckpt_dir'],'epoch-*.ckp')),\n",
    "                    key=os.path.getmtime)[-1]\n",
    "\n",
    "\n",
    "checkpoint = torch.load(ckp_path)\n",
    "print('[INFO] load checkpoints: ' + ckp_path)\n",
    "\n",
    "model_h.load_state_dict(checkpoint['model_h_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code block to sample body configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generating body configurations\n",
    "\n",
    "### concatenate depth and seg\n",
    "xs = torch.cat([depth, seg],dim=1)\n",
    "xs_n = xs.repeat(testconfig['n_samples'], 1,1,1)\n",
    "\n",
    "### model inference\n",
    "xhnr_gen= model_h.sample(xs_n)\n",
    "\n",
    "### recover to the original translation/orientation range \n",
    "xhn_gen = convert_to_3D_rot(xhnr_gen)        \n",
    "xh_gen = recover_global_T(xhn_gen, cam_intrinsic.repeat(testconfig['n_samples'],1,1), \n",
    "                          max_d.repeat(testconfig['n_samples']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we visualize the generated body configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47b6ea74816487aa7ce9ccc1278c472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 7 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## visualizing a body mesh. Note that we use WebGL, which may cause slow responses or even stuck.\n",
    "\n",
    "body_params = body_params_encapsulate(xh_gen, to_numpy=False, batched=True)\n",
    "body_params['body_pose'] = vposer.decode(body_params['body_pose'], output_type='aa').view(testconfig['n_samples'],-1)\n",
    "\n",
    "smplx_out = body_mesh_model(**body_params)\n",
    "smplx_verts = smplx_out.vertices.detach().cpu().numpy().squeeze()    \n",
    "\n",
    "cam_ext = cam_extrinsic.squeeze().detach().cpu().numpy()\n",
    "\n",
    "### create a body point cloud\n",
    "pcd_body_list = []\n",
    "for body_index in range(testconfig['n_samples']):\n",
    "    # body_index = 20\n",
    "    pcd_body = o3d.geometry.PointCloud()\n",
    "    pcd_body.points = o3d.utility.Vector3dVector(smplx_verts[body_index])\n",
    "    pcd_body = pcd_body.uniform_down_sample(every_k_points=10)\n",
    "    \n",
    "    ### perform transformation\n",
    "    pcd_body.transform(cam_ext)\n",
    "    pcd_body_list.append(pcd_body)\n",
    "    \n",
    "    \n",
    "\n",
    "### create a scene point cloud\n",
    "pcd_scene = o3d.geometry.PointCloud()\n",
    "pcd_scene.points = scene_mesh.vertices\n",
    "pcd_scene.colors = scene_mesh.vertex_colors\n",
    "pcd_scene = pcd_scene.uniform_down_sample(every_k_points=10)\n",
    "\n",
    "### create coord frame\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "        size=0.6, origin=[0, 0, 0])\n",
    "\n",
    "pcd_coord = o3d.geometry.PointCloud()\n",
    "pcd_coord.points = mesh_frame.vertices\n",
    "pcd_coord.colors = mesh_frame.vertex_colors\n",
    "pcd_coord.transform(cam_ext)\n",
    "\n",
    "\n",
    "### visualize in WebGL\n",
    "from open3d import JVisualizer\n",
    "visualizer = JVisualizer()\n",
    "visualizer.add_geometry(pcd_scene)\n",
    "visualizer.add_geometry(pcd_coord)\n",
    "for body_index in range(testconfig['n_samples']):\n",
    "    visualizer.add_geometry(pcd_body_list[body_index])\n",
    "\n",
    "visualizer.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) scene geometry-aware fitting\n",
    "\n",
    "One see that some generated body meshes are not physically plausible, either floating in the air or penetrating into the scene mesh. Therefore, we have this geometry-aware fitting to overcome these problems. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][fitting] iter=0, l_rec=0.000000, l_vposer=0.005816, l_contact=0.092927, l_collision=0.011105\n",
      "[INFO][fitting] iter=1, l_rec=0.040743, l_vposer=0.005611, l_contact=0.092199, l_collision=0.001556\n",
      "[INFO][fitting] iter=2, l_rec=0.020373, l_vposer=0.005727, l_contact=0.090522, l_collision=0.000578\n",
      "[INFO][fitting] iter=3, l_rec=0.027008, l_vposer=0.005900, l_contact=0.091292, l_collision=0.000489\n",
      "[INFO][fitting] iter=4, l_rec=0.031415, l_vposer=0.005953, l_contact=0.091844, l_collision=0.000544\n",
      "[INFO][fitting] iter=5, l_rec=0.029502, l_vposer=0.005920, l_contact=0.091751, l_collision=0.000449\n",
      "[INFO][fitting] iter=6, l_rec=0.021165, l_vposer=0.005845, l_contact=0.091504, l_collision=0.000378\n",
      "[INFO][fitting] iter=7, l_rec=0.015752, l_vposer=0.005769, l_contact=0.090933, l_collision=0.000365\n",
      "[INFO][fitting] iter=8, l_rec=0.020897, l_vposer=0.005743, l_contact=0.090512, l_collision=0.000396\n",
      "[INFO][fitting] iter=9, l_rec=0.021867, l_vposer=0.005741, l_contact=0.090311, l_collision=0.000480\n",
      "[INFO][fitting] iter=10, l_rec=0.020600, l_vposer=0.005763, l_contact=0.090259, l_collision=0.000482\n",
      "[INFO][fitting] iter=11, l_rec=0.018673, l_vposer=0.005803, l_contact=0.089680, l_collision=0.000408\n",
      "[INFO][fitting] iter=12, l_rec=0.017500, l_vposer=0.005852, l_contact=0.089614, l_collision=0.000411\n",
      "[INFO][fitting] iter=13, l_rec=0.015158, l_vposer=0.005878, l_contact=0.090040, l_collision=0.000417\n",
      "[INFO][fitting] iter=14, l_rec=0.014171, l_vposer=0.005877, l_contact=0.089916, l_collision=0.000461\n",
      "[INFO][fitting] iter=15, l_rec=0.015250, l_vposer=0.005858, l_contact=0.089676, l_collision=0.000466\n",
      "[INFO][fitting] iter=16, l_rec=0.014709, l_vposer=0.005823, l_contact=0.089498, l_collision=0.000469\n",
      "[INFO][fitting] iter=17, l_rec=0.014994, l_vposer=0.005791, l_contact=0.089534, l_collision=0.000486\n",
      "[INFO][fitting] iter=18, l_rec=0.013819, l_vposer=0.005765, l_contact=0.089552, l_collision=0.000471\n",
      "[INFO][fitting] iter=19, l_rec=0.011893, l_vposer=0.005752, l_contact=0.089407, l_collision=0.000395\n",
      "[INFO][fitting] iter=20, l_rec=0.012180, l_vposer=0.005755, l_contact=0.089454, l_collision=0.000334\n",
      "[INFO][fitting] iter=21, l_rec=0.012469, l_vposer=0.005776, l_contact=0.089506, l_collision=0.000331\n",
      "[INFO][fitting] iter=22, l_rec=0.011836, l_vposer=0.005802, l_contact=0.089636, l_collision=0.000340\n",
      "[INFO][fitting] iter=23, l_rec=0.011827, l_vposer=0.005825, l_contact=0.089763, l_collision=0.000387\n",
      "[INFO][fitting] iter=24, l_rec=0.011635, l_vposer=0.005834, l_contact=0.089663, l_collision=0.000456\n",
      "[INFO][fitting] iter=25, l_rec=0.010689, l_vposer=0.005830, l_contact=0.089600, l_collision=0.000515\n",
      "[INFO][fitting] iter=26, l_rec=0.010500, l_vposer=0.005826, l_contact=0.089437, l_collision=0.000520\n",
      "[INFO][fitting] iter=27, l_rec=0.010758, l_vposer=0.005824, l_contact=0.089248, l_collision=0.000502\n",
      "[INFO][fitting] iter=28, l_rec=0.011037, l_vposer=0.005816, l_contact=0.089295, l_collision=0.000399\n",
      "[INFO][fitting] iter=29, l_rec=0.011163, l_vposer=0.005805, l_contact=0.089404, l_collision=0.000366\n",
      "[INFO][fitting] iter=30, l_rec=0.010388, l_vposer=0.005806, l_contact=0.089433, l_collision=0.000362\n",
      "[INFO][fitting] iter=31, l_rec=0.009414, l_vposer=0.005808, l_contact=0.089457, l_collision=0.000380\n",
      "[INFO][fitting] iter=32, l_rec=0.009744, l_vposer=0.005815, l_contact=0.089451, l_collision=0.000390\n",
      "[INFO][fitting] iter=33, l_rec=0.009972, l_vposer=0.005816, l_contact=0.089488, l_collision=0.000402\n",
      "[INFO][fitting] iter=34, l_rec=0.009776, l_vposer=0.005812, l_contact=0.089554, l_collision=0.000414\n",
      "[INFO][fitting] iter=35, l_rec=0.010214, l_vposer=0.005812, l_contact=0.089445, l_collision=0.000377\n",
      "[INFO][fitting] iter=36, l_rec=0.010302, l_vposer=0.005814, l_contact=0.089471, l_collision=0.000345\n",
      "[INFO][fitting] iter=37, l_rec=0.009695, l_vposer=0.005817, l_contact=0.089477, l_collision=0.000336\n",
      "[INFO][fitting] iter=38, l_rec=0.009467, l_vposer=0.005826, l_contact=0.089496, l_collision=0.000326\n",
      "[INFO][fitting] iter=39, l_rec=0.009361, l_vposer=0.005829, l_contact=0.089435, l_collision=0.000320\n",
      "[INFO][fitting] iter=40, l_rec=0.009614, l_vposer=0.005823, l_contact=0.089356, l_collision=0.000347\n",
      "[INFO][fitting] iter=41, l_rec=0.009967, l_vposer=0.005803, l_contact=0.089355, l_collision=0.000341\n",
      "[INFO][fitting] iter=42, l_rec=0.009676, l_vposer=0.005789, l_contact=0.089446, l_collision=0.000337\n",
      "[INFO][fitting] iter=43, l_rec=0.009436, l_vposer=0.005785, l_contact=0.089483, l_collision=0.000330\n",
      "[INFO][fitting] iter=44, l_rec=0.009294, l_vposer=0.005796, l_contact=0.089402, l_collision=0.000329\n",
      "[INFO][fitting] iter=45, l_rec=0.009643, l_vposer=0.005821, l_contact=0.089290, l_collision=0.000400\n",
      "[INFO][fitting] iter=46, l_rec=0.009925, l_vposer=0.005834, l_contact=0.088995, l_collision=0.000428\n",
      "[INFO][fitting] iter=47, l_rec=0.009817, l_vposer=0.005835, l_contact=0.088956, l_collision=0.000514\n",
      "[INFO][fitting] iter=48, l_rec=0.009488, l_vposer=0.005823, l_contact=0.088905, l_collision=0.000474\n",
      "[INFO][fitting] iter=49, l_rec=0.009435, l_vposer=0.005808, l_contact=0.088847, l_collision=0.000415\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import chamfer_pytorch.dist_chamfer as ext\n",
    "\n",
    "\n",
    "def get_contact_id(body_segments_folder, contact_body_parts=['L_Hand', 'R_Hand']):\n",
    "\n",
    "    contact_verts_ids = []\n",
    "    contact_faces_ids = []\n",
    "\n",
    "    for part in contact_body_parts:\n",
    "        with open(os.path.join(body_segments_folder, part + '.json'), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            contact_verts_ids.append(list(set(data[\"verts_ind\"])))\n",
    "            contact_faces_ids.append(list(set(data[\"faces_ind\"])))\n",
    "\n",
    "    contact_verts_ids = np.concatenate(contact_verts_ids)\n",
    "    contact_faces_ids = np.concatenate(contact_faces_ids)\n",
    "\n",
    "\n",
    "    return contact_verts_ids, contact_faces_ids\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def verts_transform(verts_batch, cam_ext_batch):\n",
    "    verts_batch_homo = F.pad(verts_batch, (0,1), mode='constant', value=1)\n",
    "    verts_batch_homo_transformed = torch.matmul(verts_batch_homo,\n",
    "                                                cam_ext_batch.permute(0,2,1))\n",
    "\n",
    "    verts_batch_transformed = verts_batch_homo_transformed[:,:,:-1]\n",
    "    \n",
    "    return verts_batch_transformed    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def cal_loss(xhr, xhr_rec, cam_ext_batch, s_verts_batch,\n",
    "             s_sdf_batch,s_grid_min_batch, s_grid_max_batch,\n",
    "             lossconfig, fittingconfig):\n",
    "\n",
    "\n",
    "    ### reconstruction loss\n",
    "    loss_rec = lossconfig['weight_loss_rec']*F.l1_loss(xhr, xhr_rec)\n",
    "    xh_rec = convert_to_3D_rot(xhr_rec)\n",
    "\n",
    "    ### vposer loss\n",
    "    vposer_pose = xh_rec[:,16:48]\n",
    "    loss_vposer = lossconfig['weight_loss_vposer'] * torch.mean(vposer_pose**2)\n",
    "\n",
    "    ### contact loss\n",
    "    body_param_rec = body_params_encapsulate(xh_rec, to_numpy=False, batched=True)\n",
    "    body_param_rec['body_pose'] = vposer.decode(body_param_rec['body_pose'], \n",
    "                                       output_type='aa').view(xhr.shape[0], -1)\n",
    "\n",
    "    smplx_output = body_mesh_model(return_verts=True, **body_param_rec)\n",
    "    body_verts_batch = smplx_output.vertices #[b, 10475,3]\n",
    "    body_verts_batch = verts_transform(body_verts_batch, cam_ext_batch)\n",
    "\n",
    "    \n",
    "    vid, fid = get_contact_id(body_segments_folder=fittingconfig['body_segments_folder'],\n",
    "                              contact_body_parts=fittingconfig['contact_part'])\n",
    "    \n",
    "    body_verts_contact_batch = body_verts_batch[:, vid, :]\n",
    "\n",
    "    dist_chamfer_contact = ext.chamferDist()\n",
    "    contact_dist, _ = dist_chamfer_contact(body_verts_contact_batch.contiguous(), \n",
    "                                            s_verts_batch.contiguous())\n",
    "\n",
    "    loss_contact = lossconfig['weight_contact'] * torch.mean(torch.sqrt(contact_dist+1e-4)\n",
    "                                                            /(torch.sqrt(contact_dist+1e-4)+0.01))  \n",
    "\n",
    "\n",
    "\n",
    "    ### sdf collision loss\n",
    "    s_grid_min_batch = s_grid_min_batch.unsqueeze(1)\n",
    "    s_grid_max_batch = s_grid_max_batch.unsqueeze(1)\n",
    "\n",
    "    norm_verts_batch = (body_verts_batch - s_grid_min_batch) / (s_grid_max_batch - s_grid_min_batch) *2 -1\n",
    "    n_verts = norm_verts_batch.shape[1]\n",
    "    body_sdf_batch = F.grid_sample(s_sdf_batch.unsqueeze(1), \n",
    "                                    norm_verts_batch[:,:,[2,1,0]].view(-1, n_verts,1,1,3),\n",
    "                                    padding_mode='border')\n",
    "\n",
    "\n",
    "    # if there are no penetrating vertices then set sdf_penetration_loss = 0\n",
    "    if body_sdf_batch.lt(0).sum().item() < 1:\n",
    "        loss_sdf_pene = torch.tensor(0.0, dtype=torch.float32, device=self.device)\n",
    "    else:\n",
    "        loss_sdf_pene = body_sdf_batch[body_sdf_batch < 0].abs().mean()\n",
    "\n",
    "    loss_collision = lossconfig['weight_collision']*loss_sdf_pene\n",
    "\n",
    "    return loss_rec, loss_vposer, loss_contact, loss_collision\n",
    "\n",
    " \n",
    "    \n",
    "def fitting(xhr_in, cam_extrinsic,\n",
    "            s_verts, s_sdf, s_grid_min, s_grid_max, max_d,\n",
    "            fittingconfig, lossconfig):\n",
    "    \n",
    "    \n",
    "    batch_size = xhr_in.shape[0]\n",
    "    xhr_rec = Variable(torch.randn(batch_size,75).cuda(), requires_grad=True)\n",
    "    optimizer = optim.Adam([xhr_rec], lr=fittingconfig['init_lr_h'])\n",
    "    xhr_rec.data = xhr_in.clone()\n",
    "\n",
    "    \n",
    "    cam_ext_batch = cam_extrinsic.repeat(batch_size, 1,1)\n",
    "    max_d_batch = max_d.repeat(batch_size)\n",
    "    s_verts_batch = s_verts.repeat(batch_size, 1,1)\n",
    "    s_sdf_batch = s_sdf.repeat(batch_size, 1,1,1)\n",
    "    s_grid_min_batch = s_grid_min.repeat(batch_size, 1)\n",
    "    s_grid_max_batch = s_grid_max.repeat(batch_size, 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for ii in range(fittingconfig['num_iter']):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_rec, loss_vposer, loss_contact, loss_collision = cal_loss(xhr_in, xhr_rec, cam_ext_batch, s_verts_batch,\n",
    "                                                                         s_sdf_batch,s_grid_min_batch, s_grid_max_batch,\n",
    "                                                                         lossconfig, fittingconfig)\n",
    "        loss = loss_rec + loss_vposer + loss_contact + loss_collision\n",
    "        if fittingconfig['verbose']:\n",
    "            print('[INFO][fitting] iter={:d}, l_rec={:f}, l_vposer={:f}, l_contact={:f}, l_collision={:f}'.format(\n",
    "                                    ii, loss_rec.item(), loss_vposer.item(), \n",
    "                                    loss_contact.item(), loss_collision.item()) )\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    ### recover global translation and orientation\n",
    "    xh_rec = convert_to_3D_rot(xhr_rec)        \n",
    "\n",
    "    return xh_rec\n",
    "    \n",
    "    \n",
    "    \n",
    "fittingconfig={'init_lr_h': 0.05,\n",
    "                'num_iter': 50, \n",
    "                'contact_part': ['back','butt','L_Hand','R_Hand','L_Leg',\n",
    "                                 'R_Leg','thighs'],\n",
    "                'body_segments_folder': os.path.join(proxe_path,'body_segments'),\n",
    "                'verbose': True\n",
    "              }\n",
    "\n",
    "lossconfig={\n",
    "    'weight_loss_rec': 1,\n",
    "    'weight_loss_vposer':0.01,\n",
    "    'weight_contact': 0.1,\n",
    "    'weight_collision' : 0.5\n",
    "}\n",
    "\n",
    "\n",
    "### put scene to tensors\n",
    "s_verts = torch.tensor(scene_verts, dtype=torch.float32).cuda().unsqueeze(0)\n",
    "s_grid_min = torch.tensor(grid_min, dtype=torch.float32).cuda().unsqueeze(0)\n",
    "s_grid_max = torch.tensor(grid_max, dtype=torch.float32).cuda().unsqueeze(0)\n",
    "s_sdf = torch.tensor(sdf, dtype=torch.float32).cuda().unsqueeze(0)        \n",
    "        \n",
    "xhr_gen = recover_global_T(xhnr_gen, cam_intrinsic.repeat(testconfig['n_samples'],1,1), \n",
    "                          max_d.repeat(testconfig['n_samples']))\n",
    "\n",
    "xh_fitting = fitting(xhr_gen, cam_extrinsic,\n",
    "                    s_verts, s_sdf, s_grid_min, s_grid_max, max_d,\n",
    "                    fittingconfig, lossconfig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bab3a748c641be8c7d978c6b7304cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 7 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## visualizing a body mesh. Note that we use WebGL, which may cause slow responses or even stuck.\n",
    "\n",
    "body_params = body_params_encapsulate(xh_fitting, to_numpy=False, batched=True)\n",
    "body_params['body_pose'] = vposer.decode(body_params['body_pose'], output_type='aa').view(testconfig['n_samples'],-1)\n",
    "\n",
    "smplx_out = body_mesh_model(**body_params)\n",
    "smplx_verts = smplx_out.vertices.detach().cpu().numpy().squeeze()    \n",
    "\n",
    "cam_ext = cam_extrinsic.squeeze().detach().cpu().numpy()\n",
    "\n",
    "### create a body point cloud\n",
    "pcd_body_list = []\n",
    "for body_index in range(testconfig['n_samples']):\n",
    "    # body_index = 20\n",
    "    pcd_body = o3d.geometry.PointCloud()\n",
    "    pcd_body.points = o3d.utility.Vector3dVector(smplx_verts[body_index])\n",
    "    pcd_body = pcd_body.uniform_down_sample(every_k_points=10)\n",
    "    \n",
    "    ### perform transformation\n",
    "    pcd_body.transform(cam_ext)\n",
    "    pcd_body_list.append(pcd_body)\n",
    "    \n",
    "    \n",
    "\n",
    "### create a scene point cloud\n",
    "pcd_scene = o3d.geometry.PointCloud()\n",
    "pcd_scene.points = scene_mesh.vertices\n",
    "pcd_scene.colors = scene_mesh.vertex_colors\n",
    "pcd_scene = pcd_scene.uniform_down_sample(every_k_points=10)\n",
    "\n",
    "### create coord frame\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "        size=0.6, origin=[0, 0, 0])\n",
    "\n",
    "pcd_coord = o3d.geometry.PointCloud()\n",
    "pcd_coord.points = mesh_frame.vertices\n",
    "pcd_coord.colors = mesh_frame.vertex_colors\n",
    "pcd_coord.transform(cam_ext)\n",
    "\n",
    "\n",
    "### visualize in WebGL\n",
    "from open3d import JVisualizer\n",
    "visualizer = JVisualizer()\n",
    "visualizer.add_geometry(pcd_scene)\n",
    "visualizer.add_geometry(pcd_coord)\n",
    "for body_index in range(testconfig['n_samples']):\n",
    "    visualizer.add_geometry(pcd_body_list[body_index])\n",
    "\n",
    "visualizer.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}